% This file was created with Citavi 6.7.0.0

@book{.2003,
 year = {2003},
 title = {Encyclopedia of Food Sciences and Nutrition},
 publisher = {Elsevier},
 isbn = {9780122270550}
}


@proceedings{.2016,
 year = {2016},
 title = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 publisher = {{AAAI Press}},
 series = {AAAI'16}
}


@proceedings{.2018,
 year = {2018},
 title = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
 address = {Red Hook, NY, USA},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'18}
}


@proceedings{.2020,
 year = {2020},
 title = {Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-849-7},
 series = {AIIDE'20}
}


@proceedings{.52620185282018,
 year = {2018},
 title = {2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)},
 publisher = {IEEE},
 isbn = {978-1-5386-6987-7}
}


@proceedings{.81720218202021,
 year = {2021},
 title = {2021 IEEE Conference on Games (CoG)},
 publisher = {IEEE},
 isbn = {978-1-6654-3886-5}
}


@proceedings{.82420208272020,
 year = {2020},
 title = {2020 IEEE Conference on Games (CoG)},
 publisher = {IEEE},
 isbn = {978-1-7281-4533-4},
 doi = {10.1109/CoG47356.2020}
}


@article{Abbaspour.2021,
 author = {Abbaspour, Afshin and Jahan, Ali and Rezaiee, Marzieh},
 year = {2021},
 title = {A simple empirical model for blood platelet production and inventory management under uncertainty},
 pages = {1783--1799},
 volume = {12},
 number = {2},
 issn = {1868-5137},
 journal = {Journal of Ambient Intelligence and Humanized Computing},
 doi = {10.1007/s12652-020-02254-x}
}


@inproceedings{Bamford.2021,
 author = {Bamford, Christopher and Ovalle, Alvaro},
 title = {Generalising Discrete Action Spaces with Conditional Action Trees},
 pages = {1--8},
 publisher = {IEEE},
 isbn = {978-1-6654-3886-5},
 booktitle = {2021 IEEE Conference on Games (CoG)},
 year = {2021},
 doi = {10.1109/CoG52621.2021.9619093},
 file = {https://ieeexplore.ieee.org/document/9619093/}
}


@book{Boucherie.2017,
 year = {2017},
 title = {Markov Decision Processes in Practice},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-47764-0},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Boucherie, Richard J. and {van Dijk}, Nico M.},
 doi = {10.1007/978-3-319-47766-4}
}


@article{Boute.2021,
 author = {Boute, Robert N. and Gijsbrechts, Joren and {van Jaarsveld}, Willem and Vanvuchelen, Nathalie},
 year = {2021},
 title = {Deep reinforcement learning for inventory control: A roadmap},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 doi = {10.1016/j.ejor.2021.07.016}
}


@book{Brandeau.2005,
 year = {2005},
 title = {Operations Research and Health Care},
 address = {Boston},
 publisher = {{Kluwer Academic Publishers}},
 isbn = {1-4020-7629-0},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Brandeau, Margaret L. and Sainfort, Fran{\c{c}}ois and Pierskalla, William P.},
 doi = {10.1007/b106574}
}


@article{Cappart.2021,
 author = {Cappart, Quentin and Moisan, Thierry and Rousseau, Louis-Martin and Pr{\'e}mont-Schwarz, Isabeau and Cire, Andre A.},
 year = {2021},
 title = {Combining Reinforcement Learning and Constraint Programming for Combinatorial Optimization},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/16484},
 pages = {3677--3687},
 volume = {35},
 number = {5},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence}
}


@article{Chaudhary.2018,
 author = {Chaudhary, Vaibhav and Kulshrestha, Rakhee and Routroy, Srikanta},
 year = {2018},
 title = {State-of-the-art literature review on inventory models for perishable products},
 pages = {306--346},
 volume = {15},
 number = {3},
 issn = {0972-7981},
 journal = {Journal of Advances in Management Research},
 doi = {10.1108/JAMR-09-2017-0091}
}


@article{Chen.2021,
 author = {Chen, Shouchang and Li, Yanzhi and Yang, Yi and Zhou, Weihua},
 year = {2021},
 title = {Managing Perishable Inventory Systems with Age--differentiated Demand},
 pages = {3784--3799},
 volume = {30},
 number = {10},
 issn = {1059-1478},
 journal = {Production and Operations Management},
 doi = {10.1111/poms.13481}
}


@article{Civelek.2015,
 author = {Civelek, Ismail and Karaesmen, Itir and Scheller-Wolf, Alan},
 year = {2015},
 title = {Blood platelet inventory management with protection levels},
 pages = {826--838},
 volume = {243},
 number = {3},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 doi = {10.1016/j.ejor.2015.01.023}
}


@article{Cohen.1976,
 author = {Cohen, Morris A.},
 year = {1976},
 title = {Analysis of Single Critical Number Ordering Policies for Perishable Inventories},
 pages = {726--741},
 volume = {24},
 number = {4},
 issn = {0030-364X},
 journal = {Operations Research},
 doi = {10.1287/opre.24.4.726}
}


@article{Cohen.1981,
 author = {Cohen, Morris A. and Pierskalla, William P. and Yen, Hsiang-chun},
 year = {1981},
 title = {An analysis of ordering and allocation policies for multi-echelon, age-differentiated inventory systems},
 pages = {353--378},
 volume = {16},
 number = {1},
 journal = {TIMS studies in the Management Sciences}
}


@incollection{Cristovam.2003,
 author = {Cristovam, E. and Paterson, A.},
 title = {PORT | The Product and its Manufacture},
 pages = {4630--4638},
 publisher = {Elsevier},
 isbn = {9780122270550},
 booktitle = {Encyclopedia of Food Sciences and Nutrition},
 year = {2003},
 doi = {10.1016/B0-12-227055-X/00944-5}
}


@inproceedings{Delarue.2020,
 author = {Delarue, Arthur and Anderson, Ross and Tjandraatmadja, Christian},
 title = {Reinforcement Learning with Combinatorial Actions: An Application to Vehicle Routing},
 url = {https://proceedings.neurips.cc/paper/2020/file/06a9d51e04213572ef0720dd27a84792-Paper.pdf},
 pages = {609--620},
 volume = {33},
 publisher = {{Curran Associates, Inc}},
 editor = {{H. Larochelle} and {M. Ranzato} and {R. Hadsell} and {M. F. Balcan} and {H. Lin}},
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2020}
}


@article{Deniz.2010,
 author = {Deniz, Borga and Karaesmen, Itir and Scheller-Wolf, Alan},
 year = {2010},
 title = {Managing Perishables with Substitution: Inventory Issuance and Replenishment Heuristics},
 pages = {319--329},
 volume = {12},
 number = {2},
 issn = {1523-4614},
 journal = {Manufacturing {\&} Service Operations Management},
 doi = {10.1287/msom.1090.0276}
}


@article{Deniz.2020,
 author = {Deniz, Borga and Karaesmen, Itir and Scheller-Wolf, Alan},
 year = {2020},
 title = {A comparison of inventory policies for perishable goods},
 pages = {805--810},
 volume = {48},
 number = {6},
 issn = {01676377},
 journal = {Operations Research Letters},
 doi = {10.1016/j.orl.2020.09.005}
}


@article{DulacArnold.2016,
 author = {Dulac-Arnold, Gabriel and Evans, Richard and {van Hasselt}, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
 year = {2016},
 title = {Deep Reinforcement Learning in Large Discrete Action Spaces},
 volume = {abs/1512.07679},
 journal = {CoRR},
 file = {http://arxiv.org/abs/1512.07679}
}


@article{DulacArnold.2021,
 author = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
 year = {2021},
 title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
 pages = {2419--2468},
 volume = {110},
 number = {9},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1007/s10994-021-05961-4}
}


@article{Elman.1993,
 author = {Elman, Jeffrey L.},
 year = {1993},
 title = {Learning and development in neural networks: the importance of starting small},
 pages = {71--99},
 volume = {48},
 number = {1},
 issn = {00100277},
 journal = {Cognition},
 doi = {10.1016/0010-0277(93)90058-4}
}


@inproceedings{Farquhar.2020,
 abstract = {In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to accelerate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data, value estimates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.},
 author = {Farquhar, Gregory and Gustafson, Laura and Lin, Zeming and Whiteson, Shimon and Usunier, Nicolas and Synnaeve, Gabriel},
 title = {Growing Action Spaces},
 url = {https://proceedings.mlr.press/v119/farquhar20a.html},
 pages = {3040--3051},
 volume = {119},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {III, Hal Daum{\'e} and Singh, Aarti},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 year = {2020}
}


@inproceedings{Fedus.2020,
 abstract = {Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay --- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.},
 author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
 title = {Revisiting Fundamentals of Experience Replay},
 url = {https://proceedings.mlr.press/v119/fedus20a.html},
 pages = {3061--3071},
 volume = {119},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {III, Hal Daum{\'e} and Singh, Aarti},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 year = {2020}
}


@article{Gijsbrechts.2018,
 author = {Gijsbrechts, Joren and Boute, Robert N. and {van Mieghem}, Jan Albert and Zhang, Dennis},
 year = {2021},
 title = {Can Deep Reinforcement Learning Improve Inventory Management? Performance on Lost Sales, Dual-Sourcing, and Multi-Echelon Problems},
 issn = {1523-4614},
 journal = {Manufacturing {\&} Service Operations Management},
 doi = {10.1287/msom.2021.1064}
}


@article{Goh.1993,
 author = {Goh, Chon-Huat and Greenberg, Betsy S. and Matsuo, Hirofumi},
 year = {1993},
 title = {Two-Stage Perishable Inventory Models},
 pages = {633--649},
 volume = {39},
 number = {5},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.39.5.633}
}


@proceedings{H.Larochelle.2020,
 year = {2020},
 title = {Advances in Neural Information Processing Systems},
 publisher = {{Curran Associates, Inc}},
 editor = {{H. Larochelle} and {M. Ranzato} and {R. Hadsell} and {M. F. Balcan} and {H. Lin}}
}


@article{Haijema.2007,
 author = {Haijema, Ren{\'e} and {van der Wal}, Jan and {van Dijk}, Nico M.},
 year = {2007},
 title = {Blood platelet production: Optimization by dynamic programming and simulation},
 pages = {760--779},
 volume = {34},
 number = {3},
 issn = {03050548},
 journal = {Computers {\&} Operations Research},
 doi = {10.1016/j.cor.2005.03.023}
}


@phdthesis{Haijema.2008,
 author = {Haijema, Rene},
 year = {2008},
 title = {Solving large structured Markov Decision Problems for perishable inventory management and traffic control},
 url = {https://dare.uva.nl/search?identifier=b29a9ca7-fef1-48a5-bf42-4b56d18fdb6f},
 publisher = {Universiteit van Amsterdam},
 school = {{Amsterdam School of Economics Research Institute}},
 type = {PhD thesis}
}


@incollection{Haijema.2017,
 author = {Haijema, Rene and {van Dijk}, Nico M. and {van der Wal}, Jan},
 title = {Blood Platelet Inventory Management},
 pages = {293--317},
 volume = {248},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-47764-0},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Boucherie, Richard J. and {van Dijk}, Nico M.},
 booktitle = {Markov Decision Processes in Practice},
 year = {2017},
 address = {Cham},
 doi = {10.1007/978-3-319-47766-4{\textunderscore }10}
}


@article{Haijema.2019,
 author = {Haijema, Ren{\'e} and Minner, Stefan},
 year = {2019},
 title = {Improved ordering of perishables: The value of stock-age information},
 pages = {316--324},
 volume = {209},
 issn = {09255273},
 journal = {International Journal of Production Economics},
 doi = {10.1016/j.ijpe.2018.03.008}
}


@misc{Harsha.2021,
 author = {Harsha, Pavithra and Jagmohan, Ashish and Kalagnanam, Jayant and Quanz, Brian and Singhvi, Divya},
 year = {2021},
 title = {Math Programming based Reinforcement Learning for Multi-Echelon Inventory Management},
 address = {Appeared at the Deep Reinforcement Learning Workshop, NeurIPS 2021},
 journal = {SSRN Electronic Journal},
 doi = {10.2139/ssrn.3901070}
}


@article{Hendrix.2019,
 author = {Hendrix, E. M. T. and Ortega, G. and Haijema, R. and Buisman, M. E. and Garc{\'i}a, I.},
 year = {2019},
 title = {On computing optimal policies in perishable inventory control using value iteration},
 volume = {1},
 number = {4},
 issn = {2577-7408},
 journal = {Computational and Mathematical Methods},
 doi = {10.1002/cmm4.1027}
}


@article{Hessel.2018,
 author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
 year = {2018},
 title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
 url = {https://ojs.aaai.org/index.php/AAAI/article/view/11796},
 volume = {32},
 number = {1},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence}
}


@proceedings{IEEE.2020,
 year = {2020},
 title = {2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)},
 institution = {IEEE}
}


@proceedings{III.2020,
 year = {2020},
 title = {Proceedings of the 37th International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {III, Hal Daum{\'e} and Singh, Aarti}
}


@proceedings{III.2020b,
 year = {2020},
 title = {Proceedings of the 37th International Conference on Machine Learning},
 publisher = {PMLR},
 series = {Proceedings of Machine Learning Research},
 editor = {III, Hal Daum{\'e} and Singh, Aarti}
}


@inproceedings{Kanervisto.82420208272020,
 author = {Kanervisto, Anssi and Scheller, Christian and Hautamaki, Ville},
 title = {Action Space Shaping in Deep Reinforcement Learning},
 pages = {479--486},
 publisher = {IEEE},
 isbn = {978-1-7281-4533-4},
 booktitle = {2020 IEEE Conference on Games (CoG)},
 year = {2020},
 doi = {10.1109/CoG47356.2020.9231687},
 file = {https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=9222389}
}


@incollection{Karaesmen.2011,
 author = {Karaesmen, Itir Z. and Scheller--Wolf, Alan and Deniz, Borga},
 title = {Managing Perishable and Aging Inventories: Review and Future Research Directions},
 pages = {393--436},
 volume = {151},
 publisher = {{Springer US}},
 isbn = {978-1-4419-6484-7},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Kempf, Karl G. and Keskinocak, P{\i}nar and Uzsoy, Reha},
 booktitle = {Planning Production and Inventories in the Extended Enterprise},
 year = {2011},
 address = {New York, NY},
 doi = {10.1007/978-1-4419-6485-4{\textunderscore }15}
}


@book{Kempf.2011,
 year = {2011},
 title = {Planning Production and Inventories in the Extended Enterprise},
 address = {New York, NY},
 publisher = {{Springer US}},
 isbn = {978-1-4419-6484-7},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Kempf, Karl G. and Keskinocak, P{\i}nar and Uzsoy, Reha},
 doi = {10.1007/978-1-4419-6485-4}
}


@article{Lin.1998,
 author = {Lin, Ching-Rong and Buongiorno, Joseph},
 year = {1998},
 title = {Tree Diversity, Landscape Diversity, and Economics of Maple-Birch Forests: Implications of Markovian Models},
 pages = {1351--1366},
 volume = {44},
 number = {10},
 issn = {0025-1909},
 journal = {Management Science},
 doi = {10.1287/mnsc.44.10.1351}
}


@article{Mazyavkina.2021,
 author = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
 year = {2021},
 title = {Reinforcement learning for combinatorial optimization: A survey},
 pages = {105400},
 volume = {134},
 issn = {03050548},
 journal = {Computers {\&} Operations Research},
 doi = {10.1016/j.cor.2021.105400}
}


@article{Meisheri.2021,
 author = {Meisheri, Hardik and Sultana, Nazneen N. and Baranwal, Mayank and Baniwal, Vinita and Nath, Somjit and Verma, Satyam and Ravindran, Balaraman and Khadilkar, Harshad},
 year = {2021},
 title = {Scalable multi-product inventory control with lead time constraints using reinforcement learning},
 issn = {0941-0643},
 journal = {Neural Computing and Applications},
 doi = {10.1007/s00521-021-06129-w}
}


@article{Mnih.2015,
 abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
 author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
 year = {2015},
 title = {Human-level control through deep reinforcement learning},
 pages = {529--533},
 volume = {518},
 number = {7540},
 journal = {Nature},
 doi = {10.1038/nature14236},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25719670}
}


@article{Moor.2021,
 author = {de Moor, Bram J. and Gijsbrechts, Joren and Boute, Robert N.},
 year = {2021},
 title = {Reward shaping to improve the performance of deep reinforcement learning in perishable inventory management},
 issn = {03772217},
 journal = {European Journal of Operational Research},
 doi = {10.1016/j.ejor.2021.10.045}
}


@article{Nahmias.1976,
 author = {Nahmias, Steven and Pierskalla, William P.},
 year = {1976},
 title = {A Two-Product Perishable/Nonperishable Inventory Problem},
 pages = {483--500},
 volume = {30},
 number = {3},
 issn = {0036-1399},
 journal = {SIAM Journal on Applied Mathematics},
 doi = {10.1137/0130045}
}


@article{Nakagawa.1975,
 author = {Nakagawa, Toshio and Osaki, Shunji},
 year = {1975},
 title = {The Discrete Weibull Distribution},
 pages = {300--301},
 volume = {R-24},
 number = {5},
 issn = {0018-9529},
 journal = {IEEE Transactions on Reliability},
 doi = {10.1109/tr.1975.5214915}
}


@article{Odoni.1969,
 author = {Odoni, Amedeo R.},
 year = {1969},
 title = {On Finding the Maximal Gain for Markov Decision Processes},
 pages = {857--860},
 volume = {17},
 number = {5},
 issn = {0030-364X},
 journal = {Operations Research},
 doi = {10.1287/opre.17.5.857}
}


@article{Oroojlooyjadid.2021,
 author = {Oroojlooyjadid, Afshin and Nazari, MohammadReza and Snyder, Lawrence V. and Tak{\'a}{\v{c}}, Martin},
 year = {2021},
 title = {A Deep Q-Network for the Beer Game: Deep Reinforcement Learning for Inventory Optimization},
 issn = {1523-4614},
 journal = {Manufacturing {\&} Service Operations Management},
 doi = {10.1287/msom.2020.0939}
}


@article{Pahr.2021,
 author = {Pahr, Alexander and Grunow, Martin and Amorim, Pedro},
 year = {2021},
 title = {Deriving interpretable decision rules from optimal purchasing and blending policies in port wine inventory management}
}


@article{Panzer.2021,
 author = {Panzer, Marcel and Bender, Benedict},
 year = {2021},
 title = {Deep reinforcement learning in production systems: a systematic literature review},
 pages = {1--26},
 issn = {0020-7543},
 journal = {International Journal of Production Research},
 doi = {10.1080/00207543.2021.1973138}
}


@article{ParvezFarazi.2021,
 author = {{Parvez Farazi}, Nahid and Zou, Bo and Ahamed, Tanvir and Barua, Limon},
 year = {2021},
 title = {Deep reinforcement learning in transportation research: A review},
 pages = {100425},
 volume = {11},
 issn = {25901982},
 journal = {Transportation Research Interdisciplinary Perspectives},
 doi = {10.1016/j.trip.2021.100425}
}


@incollection{Pierskalla.2005,
 author = {Pierskalla, William P.},
 title = {Supply Chain Management of Blood Banks},
 pages = {103--145},
 volume = {70},
 publisher = {{Kluwer Academic Publishers}},
 isbn = {1-4020-7629-0},
 series = {International Series in Operations Research {\&} Management Science},
 editor = {Brandeau, Margaret L. and Sainfort, Fran{\c{c}}ois and Pierskalla, William P.},
 booktitle = {Operations Research and Health Care},
 year = {2005},
 address = {Boston},
 doi = {10.1007/1-4020-8066-2{\textunderscore }5}
}


@misc{PyTorchdocumentation.2019,
 author = {{PyTorch documentation}},
 year = {2019},
 title = {SmoothL1Loss},
 url = {https://pytorch.org/docs/1.9.1/generated/torch.nn.SmoothL1Loss.html},
 urldate = {1/12/2022}
}


@misc{PyTorchdocumentation.2019b,
 author = {{PyTorch documentation}},
 year = {2019},
 title = {Adam},
 url = {https://pytorch.org/docs/stable/generated/torch.optim.Adam.html},
 urldate = {1/12/2022}
}


@article{SanmitNarvekar.2020,
 author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E. and Stone, Peter},
 year = {2020},
 title = {Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey},
 url = {http://jmlr.org/papers/v21/20-212.html},
 pages = {1--50},
 volume = {21},
 number = {181},
 journal = {Journal of Machine Learning Research}
}


@misc{Schaul.11182015,
 abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
 author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
 date = {2015},
 title = {Prioritized Experience Replay},
 url = {http://arxiv.org/pdf/1511.05952v4},
 keywords = {Computer Science - Learning},
 file = {http://arxiv.org/abs/1511.05952v4},
 file = {https://arxiv.org/pdf/1511.05952v4.pdf}
}


@inproceedings{Seetharaman.2020,
 author = {Seetharaman, Prem and Wichern, Gordon and Pardo, Bryan and {Le Roux}, Jonathan},
 title = {AutoClip: Adaptive Gradient Clipping for Source Separation Networks},
 booktitle = {2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)},
 year = {2020}
}


@misc{Sultana.2020,
 author = {Sultana, Nazneen and Meisheri, Hardik and Baniwal, Vinita and Nath, Somjit and Ravindran, Balaraman and Khadilkar, Harshad},
 year = {2020},
 title = {Reinforcement Learning for Multi-Product Multi-Node Inventory Management in Supply Chains},
 url = {https://arxiv.org/abs/2006.04037}
}


@article{Sutton.1988,
 author = {Sutton, Richard S.},
 year = {1988},
 title = {Learning to Predict by the Methods of Temporal Differences},
 pages = {9--44},
 volume = {3},
 number = {1},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1023/A:1022633531479}
}


@book{Sutton.2018,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 year = {2018},
 title = {Reinforcement learning: An introduction},
 address = {Cambridge, Massachusetts},
 edition = {Second edition},
 publisher = {{The MIT Press}},
 isbn = {978-0-262-03924-6},
 series = {Adaptive computation and machine learning}
}


@misc{vandeWiele.2020,
 author = {{van de Wiele}, Tom and Warde-Farley, David and Mnih, Andriy and Mnih, Volodymyr},
 year = {2020},
 title = {Q-Learning in enormous action spaces via amortized approximate maximization},
 url = {https://arxiv.org/abs/2001.08116},
 address = {Appeared at the Deep Reinforcement Learning Workshop, NeurIPS 2018}
}


@inproceedings{vanHasselt.2016,
 abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
 author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
 title = {Deep Reinforcement Learning with Double Q-Learning},
 pages = {2094--2100},
 publisher = {{AAAI Press}},
 series = {AAAI'16},
 booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
 year = {2016}
}


@phdthesis{vanZyl.,
 author = {{van Zyl}, Gideon Johannes Jacobus},
 year = {1964},
 title = {Inventory control for perishable commodities},
 url = {https://repository.lib.ncsu.edu/handle/1840.4/2387},
 address = {[Chapel Hill, N.C.]},
 publisher = {Dept. of Statistics},
 school = {{North Carolina State University}}
}


@phdthesis{Watkins.1989,
 author = {Watkins, Christopher John Cornish Hellaby},
 year = {1989},
 title = {Learning from delayed rewards},
 url = {http://www.cs.rhul.ac.uk/~chrisw/thesis.html},
 publisher = {King's College},
 school = {{Cambridge, Univ.}},
 type = {Ph.D.Thesis}
}


@misc{Yang.9142021,
 abstract = {Deep Reinforcement Learning (DRL) and Deep Multi-agent Reinforcement Learning (MARL) have achieved significant success across a wide range of domains, such as game AI, autonomous vehicles, robotics and finance. However, DRL and deep MARL agents are widely known to be sample-inefficient and millions of interactions are usually needed even for relatively simple game settings, thus preventing the wide application in real-industry scenarios. One bottleneck challenge behind is the well-known exploration problem, i.e., how to efficiently explore the unknown environments and collect informative experiences that could benefit the policy learning most.  In this paper, we conduct a comprehensive survey on existing exploration methods in DRL and deep MARL for the purpose of providing understandings and insights on the critical problems and solutions. We first identify several key challenges to achieve efficient exploration, which most of the exploration methods aim at addressing. Then we provide a systematic survey of existing approaches by classifying them into two major categories: uncertainty-oriented exploration and intrinsic motivation-oriented exploration. The essence of uncertainty-oriented exploration is to leverage the quantification of the epistemic and aleatoric uncertainty to derive efficient exploration. By contrast, intrinsic motivation-oriented exploration methods usually incorporate different reward agnostic information for intrinsic exploration guidance. Beyond the above two main branches, we also conclude other exploration methods which adopt sophisticated techniques but are difficult to be classified into the above two categories. In addition, we provide a comprehensive empirical comparison of exploration methods for DRL on a set of commonly used benchmarks. Finally, we summarize the open problems of exploration in DRL and deep MARL and point out a few future directions.},
 author = {Yang, Tianpei and Tang, Hongyao and Bai, Chenjia and Liu, Jinyi and Hao, Jianye and Meng, Zhaopeng and Liu, Peng},
 year = {2021},
 title = {Exploration in Deep Reinforcement Learning: A Comprehensive Survey},
 url = {http://arxiv.org/pdf/2109.06668v2},
 file = {http://arxiv.org/abs/2109.06668v2},
 file = {https://arxiv.org/pdf/2109.06668v2.pdf}
}


@inproceedings{You.2020,
 abstract = {Deep reinforcement learning (DRL) has gained a lot of attention in recent years, and has been proven to be able to play Atari games and Go at or above human levels. However, those games are assumed to have a small fixed number of actions and could be trained with a simple CNN network. In this paper, we study a special class of Asian popular card games called Dou Di Zhu, in which two adversarial groups of agents must consider numerous card combinations at each time step, leading to huge number of actions. We propose a novel method to handle combinatorial actions, which we call combinatorial Q-learning (CQL). We employ a two-stage network to reduce action space and also leverage order-invariant max-pooling operations to extract relationships between primitive actions. Results show that our method prevails over other baseline learning algorithms like naive Q-learning and A3C. We develop an easy-to-use card game environments and train all agents adversarially from sractch, with only knowledge of game rules and verify that our agents are comparative to humans.},
 author = {You, Yang and Li, Liangwei and Guo, Baisong and Wang, Weiming and Lu, Cewu},
 title = {Combinatorial Q-Learning for Dou Di Zhu},
 publisher = {{AAAI Press}},
 isbn = {978-1-57735-849-7},
 series = {AIIDE'20},
 booktitle = {Proceedings of the Sixteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
 year = {2020}
}


@misc{you2019does,
 author = {You, Kaichao and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I.},
 year = {2019},
 title = {How Does Learning Rate Decay Help Modern Neural Networks?},
 url = {https://arxiv.org/abs/1908.01878}
}


@inproceedings{Zahavy.2018,
 abstract = {Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.},
 author = {Zahavy, Tom and Haroush, Matan and Merlis, Nadav and Mankowitz, Daniel J. and Mannor, Shie},
 title = {Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning},
 pages = {3566--3577},
 publisher = {{Curran Associates Inc}},
 series = {NIPS'18},
 booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
 year = {2018},
 address = {Red Hook, NY, USA}
}


@article{Zhang.2021,
 author = {Zhang, Tengteng and Mo, Hongwei},
 year = {2021},
 title = {Reinforcement learning for robot research: A comprehensive review and open issues},
 pages = {172988142110073},
 volume = {18},
 number = {3},
 issn = {1729-8814},
 journal = {International Journal of Advanced Robotic Systems},
 doi = {10.1177/17298814211007305}
}


@inproceedings{Zhao.52620185282018,
 author = {Zhao, Zhiheng and Liang, Yi and Jin, Xiaoming},
 title = {Handling large-scale action space in deep Q network},
 pages = {93--96},
 publisher = {IEEE},
 isbn = {978-1-5386-6987-7},
 booktitle = {2018 International Conference on Artificial Intelligence and Big Data (ICAIBD)},
 year = {2018},
 doi = {10.1109/ICAIBD.2018.8396173},
 file = {https://ieeexplore.ieee.org/document/8396173/}
}


@misc{Zheng.2018,
 author = {Zheng, Stephan and Yue, Yisong},
 year = {2018},
 title = {Structured Exploration via Hierarchical Variational Policy Networks}
}


